{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21754154",
   "metadata": {},
   "source": [
    "\n",
    "## **Langkah 1: Preprocessing Data (Tokenisasi dan Normalisasi)**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Kita mulai dengan mempersiapkan dataset dan melakukan **tokenisasi** (mengubah kalimat menjadi kata-kata) serta **normalisasi** (misalnya, semua menjadi lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d4215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['beli', 'sekarang'], 'Spam'), (['diskon', 'besar'], 'Spam'), (['saya', 'suka', 'belajar'], 'Ham'), (['belajar', 'sekarang', 'di', 'rumah'], 'Ham')]\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data = [\n",
    "    (\"beli sekarang\", \"Spam\"),\n",
    "    (\"diskon besar\", \"Spam\"),\n",
    "    (\"saya suka belajar\", \"Ham\"),\n",
    "    (\"belajar sekarang di rumah\", \"Ham\")\n",
    "]\n",
    "\n",
    "# Tokenisasi dan normalisasi (case folding)\n",
    "def preprocess(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Tokenisasi dan normalisasi untuk seluruh dataset\n",
    "processed_data = [(preprocess(text), label) for text, label in data]\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071477c8",
   "metadata": {},
   "source": [
    "\n",
    "## **Langkah 2: Membuat Vocabulary (Kamus Kata Unik)**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Kita membuat **vocabulary** yang berisi kata-kata unik yang ada di dataset. Ini akan menjadi fitur untuk representasi teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65684de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suka', 'besar', 'beli', 'sekarang', 'rumah', 'di', 'saya', 'diskon', 'belajar']\n"
     ]
    }
   ],
   "source": [
    "# Membuat vocabulary\n",
    "vocab = set(word for text, _ in processed_data for word in text)\n",
    "vocab = list(vocab)  # Menjadikan vocab sebagai list\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16e8e4",
   "metadata": {},
   "source": [
    "\n",
    "## **Langkah 3: Representasi Data dengan Bag of Words (BoW)**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Kita mengubah setiap kalimat menjadi vektor **Bag of Words (BoW)**, di mana setiap elemen vektor mewakili frekuensi kemunculan kata dalam kalimat berdasarkan vocabulary yang telah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 1, 1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk representasi BoW\n",
    "def text_to_bow(text, vocab):\n",
    "    return [text.count(word) for word in vocab]\n",
    "\n",
    "# Menerapkan BoW ke setiap teks dalam dataset\n",
    "X = [text_to_bow(text, vocab) for text, _ in processed_data]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ddc05",
   "metadata": {},
   "source": [
    "\n",
    "## **Langkah 4: Menghitung Probabilitas Kelas (P(Spam) dan P(Ham))**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Hitung probabilitas masing-masing kelas (`Spam` dan `Ham`). Ini adalah **P(Spam)** dan **P(Ham)** yang dibutuhkan dalam rumus Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231a459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam) = 0.5\n",
      "P(Ham) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Menghitung probabilitas kelas\n",
    "from collections import Counter\n",
    "\n",
    "labels = [label for _, label in processed_data]\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "P_spam = label_counts['Spam'] / len(labels)\n",
    "P_ham = label_counts['Ham'] / len(labels)\n",
    "\n",
    "print(f\"P(Spam) = {P_spam}\")\n",
    "print(f\"P(Ham) = {P_ham}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505904c",
   "metadata": {},
   "source": [
    "## **Langkah 5: Menghitung Likelihood (P(w|C)) untuk setiap kata**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Untuk setiap kata dalam vocabulary, kita akan menghitung **P(w|Spam)** dan **P(w|Ham)** menggunakan **Laplace smoothing** agar tidak ada nilai probabilitas yang nol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d732339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|Spam): {'suka': 0.07692307692307693, 'besar': 0.15384615384615385, 'beli': 0.15384615384615385, 'sekarang': 0.15384615384615385, 'rumah': 0.07692307692307693, 'di': 0.07692307692307693, 'saya': 0.07692307692307693, 'diskon': 0.15384615384615385, 'belajar': 0.07692307692307693}\n",
      "P(w|Ham): {'suka': 0.125, 'besar': 0.0625, 'beli': 0.0625, 'sekarang': 0.125, 'rumah': 0.125, 'di': 0.125, 'saya': 0.125, 'diskon': 0.0625, 'belajar': 0.1875}\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghitung P(w|C) menggunakan Laplace Smoothing\n",
    "def compute_likelihood(class_data, vocab, total_words_in_class, vocab_size):\n",
    "    word_counts = Counter(word for text in class_data for word in text)\n",
    "    likelihood = {}\n",
    "    \n",
    "    for word in vocab:\n",
    "        count = word_counts[word] + 1  # Laplace smoothing\n",
    "        prob = count / (total_words_in_class + vocab_size)\n",
    "        likelihood[word] = prob\n",
    "        \n",
    "    return likelihood\n",
    "\n",
    "# Pisahkan data berdasarkan kelas\n",
    "spam_data = [text for text, label in processed_data if label == 'Spam']\n",
    "ham_data = [text for text, label in processed_data if label == 'Ham']\n",
    "\n",
    "# Total kata dalam Spam dan Ham\n",
    "total_spam_words = sum(len(text) for text in spam_data)\n",
    "total_ham_words = sum(len(text) for text in ham_data)\n",
    "\n",
    "# Hitung P(w|Spam) dan P(w|Ham)\n",
    "P_w_given_spam = compute_likelihood(spam_data, vocab, total_spam_words, len(vocab))\n",
    "P_w_given_ham = compute_likelihood(ham_data, vocab, total_ham_words, len(vocab))\n",
    "\n",
    "print(\"P(w|Spam):\", P_w_given_spam)\n",
    "print(\"P(w|Ham):\", P_w_given_ham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699680",
   "metadata": {},
   "source": [
    "\n",
    "## **Langkah 6: Prediksi Kelas untuk Kalimat Baru**\n",
    "\n",
    "### **Proses**:\n",
    "\n",
    "Kita akan menggunakan rumus Naive Bayes untuk menghitung probabilitas kelas **Spam** dan **Ham** untuk kalimat baru `\"sekarang belajar\"`. Pilih kelas dengan probabilitas tertinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b67d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi: Ham\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Fungsi untuk menghitung P(C|x) menggunakan Naive Bayes\n",
    "def predict(text, P_spam, P_ham, P_w_given_spam, P_w_given_ham, vocab):\n",
    "    # Representasi BoW untuk teks\n",
    "    text_bow = text_to_bow(text, vocab)\n",
    "    \n",
    "    # Hitung P(Spam | x) dan P(Ham | x)\n",
    "    P_spam_x = math.log(P_spam)  # P(Spam)\n",
    "    P_ham_x = math.log(P_ham)    # P(Ham)\n",
    "    \n",
    "    for word, count in zip(vocab, text_bow):\n",
    "        if count > 0:\n",
    "            P_spam_x += math.log(P_w_given_spam[word]) * count\n",
    "            P_ham_x += math.log(P_w_given_ham[word]) * count\n",
    "            \n",
    "    return 'Spam' if P_spam_x > P_ham_x else 'Ham'\n",
    "\n",
    "# Kalimat baru yang ingin diprediksi\n",
    "new_text = [\"sekarang\", \"belajar\"]\n",
    "\n",
    "# Prediksi kelasnya\n",
    "prediction = predict(new_text, P_spam, P_ham, P_w_given_spam, P_w_given_ham, vocab)\n",
    "print(f\"Prediksi: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68507bde",
   "metadata": {},
   "source": [
    "\n",
    "### **Kesimpulan**:\n",
    "\n",
    "-   Kalimat baru `\"sekarang belajar\"` diklasifikasikan sebagai **Ham** (bukan spam).\n",
    "    \n",
    "-   Proses dimulai dengan **preprocessing**, lalu membuat **vocabulary**, dan menghitung **probabilitas kelas** dan **likelihood**.\n",
    "    \n",
    "-   **Naive Bayes** digunakan untuk menghitung probabilitas dan memilih kelas dengan probabilitas tertinggi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
